{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-layer Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define activation functions and derivatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base class for activation functions\n",
    "class ActivationFunction:\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def derivative(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ReLU Activation Function\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        sigmoid_x = self.__call__(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T04:09:39.534741Z",
     "start_time": "2024-03-19T04:09:39.530990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Multilayer neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MultiLayerNet:\n",
    "    def __init__(self, layer_sizes, activation_funcs, loss_function, reg_lambda=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_funcs = activation_funcs\n",
    "        self.loss_function = loss_function\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.params = {}\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.params[f'W{i}'] = (\n",
    "                    np.random.randn(layer_sizes[i-1], \n",
    "                                    layer_sizes[i]) / \n",
    "                    np.sqrt(layer_sizes[i-1]))\n",
    "            self.params[f'b{i}'] = np.zeros((1, layer_sizes[i]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        cache = {'A0': X}\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
    "            Z = np.dot(cache[f'A{i-1}'], W) + b\n",
    "            activation_func = self.activation_funcs[i-1]\n",
    "            A = activation_func(Z)\n",
    "            cache[f'Z{i}'] = Z\n",
    "            cache[f'A{i}'] = A\n",
    "        return A, cache\n",
    "\n",
    "    def backward(self, cache, Y):\n",
    "        grads = {}\n",
    "        output = cache[f'A{len(self.layer_sizes)-1}']\n",
    "        dA_prev = self.loss_derivative(Y, output)\n",
    "        \n",
    "        for i in reversed(range(1, len(self.layer_sizes))):\n",
    "            dZ = (dA_prev * \n",
    "                  self.activation_funcs[i-1].derivative(cache[f'Z{i}']))\n",
    "            dA_prev = np.dot(dZ, self.params[f'W{i}'].T)\n",
    "            grads[f'dW{i}'] = (np.dot(cache[f'A{i-1}'].T, dZ) + \n",
    "                               self.reg_lambda * self.params[f'W{i}'])\n",
    "            grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.params[f'W{i}'] -= learning_rate * grads[f'dW{i}']\n",
    "            self.params[f'b{i}'] -= learning_rate * grads[f'db{i}']\n",
    "\n",
    "    # Adding a simple MSE loss function for demonstration\n",
    "    def compute_loss(self, Y, output):\n",
    "        return np.mean(np.square(Y - output))\n",
    "    \n",
    "    # Derivative of MSE loss with respect to output\n",
    "    def loss_derivative(self, Y, output):\n",
    "        return 2 * (output - Y) / Y.size\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            output, cache = self.forward(X)\n",
    "            grads = self.backward(cache, Y)\n",
    "            self.update_params(grads, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.compute_loss(Y, output)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T04:11:45.177862Z",
     "start_time": "2024-03-19T04:11:45.162720Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 17.329284265847377\n",
      "Epoch 100, Loss: 13.80542313681342\n",
      "Epoch 200, Loss: 13.516063014546582\n",
      "Epoch 300, Loss: 13.45837820447229\n",
      "Epoch 400, Loss: 13.436331545397715\n",
      "Epoch 500, Loss: 13.425149590372207\n",
      "Epoch 600, Loss: 13.41851624253497\n",
      "Epoch 700, Loss: 13.414180419127522\n",
      "Epoch 800, Loss: 13.411146367981829\n",
      "Epoch 900, Loss: 13.408920308228062\n",
      "Epoch 1000, Loss: 13.407225978470265\n",
      "Epoch 1100, Loss: 13.405898362495073\n",
      "Epoch 1200, Loss: 13.404833656333055\n",
      "Epoch 1300, Loss: 13.403964073058132\n",
      "Epoch 1400, Loss: 13.403242769599933\n",
      "Epoch 1500, Loss: 13.402636307371633\n",
      "Epoch 1600, Loss: 13.402120490638245\n",
      "Epoch 1700, Loss: 13.401677427614464\n",
      "Epoch 1800, Loss: 13.401293449234881\n",
      "Epoch 1900, Loss: 13.400958094163787\n"
     ]
    }
   ],
   "source": [
    "# Sample data generation for a simple linear relationship: y = 2x1 - 3x2 + 5\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X_train = np.random.rand(100, 2)\n",
    "Y_train = 2*X_train[:, 0] - 3*X_train[:, 1] + 5\n",
    "Y_train = Y_train.reshape(-1, 1)  \n",
    "# Reshape for consistency with our network's expected input\n",
    "\n",
    "# Initialize the network\n",
    "layer_sizes = [2, 4, 1]  \n",
    "# Input layer (2 neurons), \n",
    "# one hidden layer (4 neurons), \n",
    "# output layer (1 neuron)\n",
    "\n",
    "activation_funcs = [ReLU(), Sigmoid()]  \n",
    "# Using ReLU for hidden layers and Sigmoid for the output layer\n",
    "\n",
    "nn = MultiLayerNet(layer_sizes, activation_funcs, loss_function=None)\n",
    "\n",
    "# Training the network\n",
    "nn.train(X_train, Y_train, epochs=2000, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T04:11:47.277179Z",
     "start_time": "2024-03-19T04:11:47.109201Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Improvements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mini-batch training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch training\n",
    "batch_size = 64\n",
    "num_batches = len(X) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Select a random batch of data\n",
    "        batch_mask = np.random.choice(len(X), batch_size)\n",
    "        X_batch = X[batch_mask]\n",
    "        y_batch = y[batch_mask]\n",
    "\n",
    "        # Forward and backward propagation using the batch data\n",
    "        # ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimization\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "eps = 1e-8\n",
    "mW1, vW1 = 0, 0\n",
    "mW2, vW2 = 0, 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward and backward propagation\n",
    "    # ...\n",
    "    # Update parameters using Adam optimization\n",
    "    mW1 = beta1 * mW1 + (1 - beta1) * dW1\n",
    "    vW1 = beta2 * vW1 + (1 - beta2) * (dW1 ** 2)\n",
    "    mW2 = beta1 * mW2 + (1 - beta1) * dW2\n",
    "    vW2 = beta2 * vW2 + (1 - beta2) * (dW2 ** 2)\n",
    "    self.params['W1'] -= learning_rate * mW1 / (np.sqrt(vW1) + eps)\n",
    "    self.params['b1'] -= learning_rate * db1\n",
    "    self.params['W2'] -= learning_rate * mW2 / (np.sqrt(vW2) + eps)\n",
    "    self.params['b2'] -= learning_rate * db2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
