{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Activation Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4d9115fa27b55bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sigmoid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1885e81425fc7aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba931544db5e56d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tanh"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d891019b70a062c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a0c6237eced150"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ReLU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da23710b03416d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "862aa5dfa0952bbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Leaky ReLU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8947e3edf4693c6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1.0, alpha)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb4f2636e6571d7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Softmax\n",
    "The derivative of softmax is a bit more complex, as it requires computing a Jacobian matrix for the full vector output.\n",
    "\n",
    "In practice, we often use the output of the softmax function directly in the loss function, which simplifies the gradient computation during backpropagation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab8e479b2fa484cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def softmax_cross_entropy_derivative(y_true, z):\n",
    "    y_pred = softmax(z)\n",
    "    return y_pred - y_true"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd707237dd492f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d233f0f56239aad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mean squared error (MSE) loss\n",
    "##### Definition: $MSE(y, \\hat{y})=\\frac{1}{n}\\Sigma_i(y_i-\\hat{y}_i)^2$\n",
    "##### Derivative: $\\frac{\\partial MSE}{\\partial \\hat{y}}=\\frac{2}{n}(\\hat{y} - y)$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0904f88f033680b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return (2 / y_true.size) * (y_pred - y_true)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ca5de997a18b0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary cross-entropy loss\n",
    "##### Definition: $-\\frac{1}{n}\\Sigma_{i=1}[y_ilog(\\hat{y}_i)+(1-y_i)log(1-\\hat{y}_i)]$\n",
    "##### Derivative: $-\\frac{y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79154fad370a03db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categorical cross-entropy loss\n",
    "##### Definition: $-\\Sigma_i y_i log(\\hat{y}_i)$\n",
    "##### Derivative: $-\\frac{y}{\\hat{y}}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cfb60dd438cd37e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1aa02fbd245559"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L1 regularization\n",
    "The loss function becomes $L_{reg}=L+\\lambda\\Sigma_w|w|$\n",
    "\n",
    "The gradient would be $\\frac{\\partial L_{reg}}{\\partial w}=\\frac{\\partial L}{\\partial w}+\\lambda \\cdot sign(w)$, where $sign(w)$ is a function that return -1 if w<0, and 0 if w=0, and 1 if w>0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0333ab3a11dfe86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def l1_regularization(weights, lambda_reg):\n",
    "    # L1 Regularization term\n",
    "    l1_term = lambda_reg * np.sum(np.abs(weights))\n",
    "    return l1_term\n",
    "\n",
    "def l1_regularization_derivative(weights, lambda_reg):\n",
    "    # L1 Regularization derivative\n",
    "    l1_derivative = lambda_reg * np.sign(weights)\n",
    "    return l1_derivative"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3b07d118125f01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L2 regularization\n",
    "The loss function becomes $L_{reg}=L+\\frac{\\lambda}{2}\\Sigma_w w^2$\n",
    "\n",
    "The gradient would be $\\frac{\\partial L_{reg}}{\\partial w}=\\frac{\\partial L}{\\partial w}+\\lambda w$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e55f88830e4fe87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lambda_reg):\n",
    "    # L2 Regularization term\n",
    "    l2_term = lambda_reg * np.sum(np.square(weights))\n",
    "    return l2_term\n",
    "\n",
    "def l2_regularization_derivative(weights, lambda_reg):\n",
    "    # L2 Regularization derivative\n",
    "    l2_derivative = 2 * lambda_reg * weights\n",
    "    return l2_derivative"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b411344179f607c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
