{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98199bf2b4a3554"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\"\"\"Classification loss\"\"\"\n",
    "loss_fn = nn.BCELoss()  # binary classification\n",
    "loss_fn = nn.CrossEntropyLoss()  # multi-class classification\n",
    "\n",
    "\n",
    "\"\"\"Regression loss\"\"\"\n",
    "loss_fn = nn.L1Loss()  # mae loss\n",
    "loss_fn = nn.MSELoss()  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1535cf8f1fe8e73d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be24706db85c7ab6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import  optim\n",
    "\n",
    "\"\"\"optimize full parameters\"\"\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\"\"\"optimize partial parameters\"\"\"\n",
    "optimizer = optim.Adam([var1, var2], lr=0.0001)\n",
    "optimizer = optim.SGD([\n",
    "                {'params': model.base.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "            ], lr=1e-2, momentum=0.9)\n",
    "\n",
    "\"\"\"different optimizers\"\"\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=learning_rate, \n",
    "                            momentum=0.8)\n",
    "optimizer = torch.optim.Rprop(model.parameters(), \n",
    "                              lr=learning_rate,\n",
    "                              etas=(0.5, 1.2))\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), \n",
    "                                lr=learning_rate,\n",
    "                                alpha=0.99,\n",
    "                                momentum=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             betas=(0.9, 0.999))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=learning_rate,\n",
    "                              betas=(0.9, 0.999))\n",
    "\n",
    "\"\"\"Common hyperparameters\n",
    "- weight_decay (float, default 0): l2 penalty to weights\n",
    "- momentum in SGD, RMSprop:\n",
    "- etas in Rprop: \n",
    "- betas for Adam, AdamW: \n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00fcf4830151b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## learning rate decay"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b3eef97059efa3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\"\"\"Define schedular\"\"\"\n",
    "# This scheduler multiplies the learning rate by 0.1 every 30 epochs\n",
    "#         >>> # lr = 0.05     if epoch < 30\n",
    "#         >>> # lr = 0.005    if 30 <= epoch < 60\n",
    "#         >>> # lr = 0.0005   if 60 <= epoch < 90\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\"\"\"Different schedular\"\"\"\n",
    "# Decays the learning rate of each parameter group by gamma \n",
    "# once the number of epoch reaches one of the milestones.\n",
    "# >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "#         >>> # lr = 0.05     if epoch < 10\n",
    "#         >>> # lr = 0.005    if 10 <= epoch < 30\n",
    "#         >>> # lr = 0.0005   if epoch >= 30\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=[10, 30])\n",
    "\n",
    "\"\"\"Check last learning rate\"\"\"\n",
    "scheduler.get_last_lr()[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74428e287abdc998"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Usage\"\"\"\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    ...\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Learning rate decay\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de8127f49fb6c533"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Early Stopping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5375626825a510cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: How long to wait after last time validation loss improved.\n",
    "        :param verbose: If True, prints a message for each validation loss improvement. \n",
    "        :param delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "512370095ce0cf54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Usage\"\"\"\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass, backward pass, and update model parameters\n",
    "    ...\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Call early stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c82c3451ca9a0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4129936771e8c308"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define models and settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4652f851a0796468"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Define hyper-parameters\"\"\"\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "\"\"\"Load model\"\"\"\n",
    "model = Model()\n",
    "\n",
    "\"\"\"Initialize the loss function\"\"\"\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\"Select optimizer\"\"\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7dba6600a22805e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa0e0b6451abf76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Define training loop\"\"\"\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2456eeb7f9c6183c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "622c6432056a647d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Testing loop\"\"\"\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eda23e7a452a305"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save and Load Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e9425481065977a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Save model with only weights\"\"\"\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\"\"\"Load model with only weights\"\"\"\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74065f053ed16ea2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Save model with shapes\"\"\"\n",
    "torch.save(model, 'model.pth')\n",
    "model = torch.load('model.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d4393e637f2b24e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
